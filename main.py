from flask import Flask, request, jsonifyfrom flask_cors import CORSimport prawimport google.generativeai as genaifrom pytrends.request import TrendReqimport spacyfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzerimport loggingfrom dotenv import load_dotenv  # Import dotenvimport osload_dotenv()# Initialize Flask appapp = Flask(__name__)app.logger.setLevel(logging.DEBUG)# Enable CORS for your frontend URLCORS(app, resources={r"/*": {"origins": ["https://fluid-kiln-309218.web.app"]}})# Configure Reddit APIreddit = praw.Reddit(    client_id=os.getenv("REDDIT_CLIENT_ID"),    client_secret=os.getenv("REDDIT_CLIENT_SECRET"),    user_agent=os.getenv("REDDIT_USER_AGENT"))# Configure Google Generative AI APIgenai.configure(api_key=os.getenv("GOOGLE_GENERATIVE_AI_API_KEY"))# Load spaCy and VADER for NLP tasksnlp = spacy.load("en_core_web_sm")analyzer = SentimentIntensityAnalyzer()# Sentiment Analysis Helperdef analyze_sentiment(text):    sentiment_scores = analyzer.polarity_scores(text)    if sentiment_scores['compound'] > 0.1:        return 'Positive'    elif sentiment_scores['compound'] < -0.1:        return 'Negative'    else:        return 'Neutral'# Gemini API Route@app.route('/gemini', methods=['POST'])def gemini():    try:        data = request.json        trends = data.get("trends", "")        if not trends:            return jsonify({"error": "No trends provided"}), 400        prompt = f"""        Analyze the following list of keywords and identify the best business-oriented niches.        Keywords: {trends}        Provide a ranked list of the top three keywords with the strongest potential for building a profitable and scalable business.        """        model = genai.GenerativeModel('models/gemini-1.5-pro-002')        response = model.generate_content(contents=prompt)        if hasattr(response, 'candidates') and len(response.candidates) > 0:            generated_text = response.candidates[0].content.parts[0].text            cleaned_response = [                segment.strip().replace("*", "").replace("**", "")                for segment in generated_text.split("\n\n")                if segment.strip()            ]            return jsonify({"response": cleaned_response})        return jsonify({"error": "No candidates found in the response."}), 500    except Exception as e:        return jsonify({"error": f"Error processing data with Gemini API: {str(e)}"}), 500# Reddit Subreddit Search@app.route("/search_subreddits", methods=["POST"])def search_subreddits():    try:        data = request.json        keywords = data.get("keywords", [])        limit = data.get("limit", 5)        if not keywords:            return jsonify({"error": "No keywords provided"}), 400        results = {}        for keyword in keywords:            subreddits = reddit.subreddits.search(keyword)            subreddit_list = []            for subreddit in subreddits:                if len(subreddit_list) >= limit:                    break                subreddit_list.append({                    "name": subreddit.display_name,                    "title": subreddit.title,                    "subscribers": subreddit.subscribers,                    "url": f"https://www.reddit.com{subreddit.url}",                })            results[keyword] = subreddit_list        return jsonify({"data": results})    except Exception as e:        return jsonify({"error": f"Error fetching subreddits: {str(e)}"}), 500# Trending Searches from Google Trends@app.route("/trending_searches", methods=["GET"])def trending_searches():    try:        pytrends = TrendReq(hl="en-GB", tz=0)        trending_data = pytrends.trending_searches(pn="united_kingdom")        trends = trending_data[0].tolist()        return jsonify({"trending_searches": trends})    except Exception as e:        return jsonify({"error": f"Error fetching trending searches: {str(e)}"}), 500# Subreddit Insightsfrom collections import Counter@app.route('/subreddit_insights', methods=['POST'])def subreddit_insights():    """    Fetch subreddit insights based on user input.    """    try:        data = request.json        subreddit_name = data.get('subreddit', '').strip()        if not subreddit_name:            return jsonify({"error": "No subreddit provided"}), 400        # Fetch subreddit details        subreddit = reddit.subreddit(subreddit_name)        subreddit.id  # Ensure the subreddit exists        # Subreddit basic details        description = subreddit.public_description or "No description available."        subscribers = subreddit.subscribers        active_users = subreddit.accounts_active or 0        # Fetch top posts        top_posts = []        upvotes = []        total_comments = 0        text_content = []  # Collect text for keyword extraction        common_questions = []        for post in subreddit.top(limit=10, time_filter="week"):            upvotes.append(post.score)            total_comments += post.num_comments            text_content.append(post.title + " " + post.selftext)            if "?" in post.title:  # Extract questions                common_questions.append(post.title)            top_posts.append({                "title": post.title,                "url": post.url,                "upvotes": post.score,                "num_comments": post.num_comments,                "excerpt": post.selftext[:100] + "..." if post.selftext else ""            })        # Calculate metrics        average_upvotes = sum(upvotes) / len(upvotes) if upvotes else 0        comments_to_posts_ratio = total_comments / len(top_posts) if top_posts else 0        # Extract keywords using Counter        words = " ".join(text_content).split()        keywords_counter = Counter([word.lower() for word in words if word.isalpha()])        keywords = [word for word, _ in keywords_counter.most_common(10)]        # Simple sentiment analysis        sentiment_scores = analyzer.polarity_scores(" ".join(text_content))        sentiment = "Positive" if sentiment_scores['compound'] > 0.1 else "Negative" if sentiment_scores['compound'] < -0.1 else "Neutral"        # Compile results        result = {            'subreddit': subreddit.display_name,            'description': description,            'subscribers': subscribers,            'active_users': active_users,            'average_upvotes': round(average_upvotes, 2),            'comments_to_posts_ratio': round(comments_to_posts_ratio, 2),            'top_posts': top_posts,            'keywords': keywords,            'sentiment': sentiment,            'common_questions': common_questions[:5]        }        return jsonify({'data': result})    except Exception as e:        return jsonify({"error": f"Error fetching subreddit insights: {str(e)}"}), 500# Run the Flask appif __name__ == "__main__":    app.run(host="0.0.0.0", port=8080)